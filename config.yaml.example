# LLM Compression Proxy Configuration
# ===================================

# Target API endpoint to forward compressed requests to
# This is where the proxy will send requests after compression
target_api:
  url: "https://api.kimi.com/coding/"
  # Alternative endpoints for different providers:
  # anthropic: "https://api.anthropic.com"
  # openai: "https://api.openai.com"
  # z_ai: "https://api.z.ai"
  # kimi: "https://api.moonshot.cn"
  # local: "http://localhost:11434"  # For Ollama

# API Accounts for Automatic Failover
# ====================================
# Configure multiple API accounts for automatic failover when one hits rate limits.
# When a 429 error is received, the proxy will automatically switch to the next account.
accounts:
  # Primary account
  - id: primary
    name: "Primary Account"
    api_key: "sk-ant-your-primary-api-key-here"
    endpoint: null  # Uses target_api.url above, or specify custom endpoint
    priority: 1
    max_retries: 3
    retry_delay_ms: 1000

  # Backup account (optional - enables automatic failover)
  # You can specify a different endpoint for each account (e.g., different provider)
  # - id: backup
  #   name: "Backup Account"
  #   api_key: "sk-ant-your-backup-api-key-here"
  #   endpoint: "https://api.z.ai"  # Custom endpoint for this account
  #   priority: 2
  #   max_retries: 3
  #   retry_delay_ms: 1000

  # Tertiary account (optional - example with environment variable)
  # - id: tertiary
  #   name: "Tertiary Account"
  #   api_key: "${BACKUP_API_KEY}"
  #   endpoint: "https://api.moonshot.cn"  # Kimi endpoint
  #   priority: 3

# Failover Settings
# =================
# Controls how the proxy switches between accounts during rate limits
failover:
  # Enable automatic failover (requires 2+ accounts)
  enabled: true

  # Account selection strategy:
  # - "priority": Use accounts in priority order (1, 2, 3...)
  # - "round_robin": Rotate through accounts evenly
  strategy: "priority"

  # Maximum retry attempts before giving up
  max_retries: 3

  # Exponential backoff multiplier
  backoff_multiplier: 2.0

  # Maximum backoff time in seconds
  max_backoff_seconds: 300

  # Default cooldown when account is rate-limited (if API doesn't provide retry-after)
  # Once an account fails, we won't retry it for this long (default: 1 hour)
  # This ensures we "stick" with the working account instead of constantly retrying failed ones
  cooldown_seconds: 3600  # 1 hour

# Compression settings
compression:
  # Level: "low", "medium", or "high"
  level: "high"

  # Whether to compress system prompts
  compress_system: true

  # Whether to compress user messages
  compress_user: true

  # Whether to compress assistant messages (usually not needed)
  compress_assistant: false

  # DEBUG MODE: Set to true to disable compression and just pass through
  # Useful for verifying the proxy forwarding works before enabling compression
  passthrough_mode: false

  # EXPERIMENTAL: Path compression (may cause issues with tool calls)
  # Compresses file paths, URLs, and deduplicates repeated strings
  # Set to true if you paste lots of code/logs with long paths
  enable_path_compression: true

# Proxy server settings
server:
  # Port to listen on
  port: 8080

  # Host to bind to (use 127.0.0.1 for localhost only, 0.0.0.0 for all interfaces)
  host: "127.0.0.1"

  # Request timeout in seconds
  timeout: 300

# Logging settings
logging:
  # Log level: "debug", "info", "warning", "error"
  level: "debug"

  # Log compression stats for each request
  log_compression: true

  # Log full request/response bodies (for debugging)
  log_bodies: true

  # Log each step of request processing
  log_steps: true

# Metrics settings
metrics:
  # Enable metrics endpoint
  enabled: true

  # Endpoint path
  path: "/metrics"

# Provider Prompt Caching (Anthropic)
# ===================================
# Enables 90% cost reduction on repeated content
# First request "writes" cache, subsequent "read" at 0.1x cost
cache_control:
  # Enable prompt caching
  enabled: true

  # Which message roles to mark as cacheable
  cache_roles:
    - "system"      # System prompts are perfect for caching

  # Cache first N messages of conversation (static context)
  # Set to 0 to disable, or 2-3 to cache initial context
  cache_first_n_messages: 2

# Provider-specific settings
providers:
  # Anthropic API settings
  anthropic:
    api_version: "2023-06-01"

  # OpenAI API settings
  openai:
    # No special version header needed

  # Kimi API settings
  kimi:
    # Kimi uses Anthropic-compatible format
    # No special configuration needed
